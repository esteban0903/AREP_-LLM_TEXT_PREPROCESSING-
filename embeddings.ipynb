{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0850183",
   "metadata": {},
   "source": [
    "# Embeddings y procesamiento de texto para LLMs\n",
    "\n",
    "Este notebook sigue el flujo del Capítulo 2 de 'Build a Large Language Model (From Scratch)' de Sebastian Raschka, adaptando el código y explicaciones en español y en mi propio estilo. Aquí se exploran los pasos clave para procesar texto y generar embeddings, fundamentales para modelos de lenguaje y sistemas agenticos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a600e362",
   "metadata": {},
   "source": [
    "## 1. Importar librerías necesarias\n",
    "\n",
    "En este paso importamos las librerías fundamentales para el procesamiento de texto y la construcción de embeddings. Usaremos `torch` para las redes neuronales y `tiktoken` para la tokenización eficiente, además de utilidades estándar de Python.\n",
    "\n",
    "> Importar correctamente estas librerías es esencial para aprovechar las capacidades modernas de procesamiento y modelado de lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889bde84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "print('torch:', torch.__version__)\n",
    "print('tiktoken:', tiktoken.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f22321",
   "metadata": {},
   "source": [
    "## 2. Descargar y cargar el texto\n",
    "\n",
    "En este paso cargamos el archivo 'the-verdict.txt', que contiene el texto base para el procesamiento. Tener el texto localmente permite reproducibilidad y control sobre los datos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b7d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "print('Total de caracteres:', len(raw_text))\n",
    "print(raw_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ae9d1",
   "metadata": {},
   "source": [
    "## 3. Tokenización del texto\n",
    "\n",
    "Utilizamos `tiktoken` para convertir el texto en una secuencia de tokens numéricos. Esta representación es la base para que el modelo pueda trabajar con el texto.\n",
    "\n",
    "> La tokenización eficiente es clave para que el modelo entienda la estructura y el significado del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cadbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "tokens = tokenizer.encode(raw_text)\n",
    "print('Total de tokens:', len(tokens))\n",
    "print('Primeros 20 tokens:', tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b26d7f",
   "metadata": {},
   "source": [
    "## 4. División del texto en fragmentos (chunks)\n",
    "\n",
    "Dividimos la secuencia de tokens en fragmentos de longitud fija usando `max_length` y `stride`. El solapamiento (overlap) entre fragmentos permite que el modelo tenga acceso a contexto compartido entre diferentes partes del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe001a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_tokens(tokens, max_length=128, stride=64):\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens) - max_length + 1, stride):\n",
    "        chunk = tokens[i:i+max_length]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "max_length = 128\n",
    "stride = 64\n",
    "chunks = chunk_tokens(tokens, max_length, stride)\n",
    "print(f'Fragmentos generados: {len(chunks)}')\n",
    "print('Primer fragmento:', chunks[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152510f8",
   "metadata": {},
   "source": [
    "## 5. Construcción de embeddings\n",
    "\n",
    "Para cada fragmento, generamos embeddings usando una capa de embedding de `torch`. Esto transforma los IDs de tokens en vectores densos que capturan relaciones semánticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1747a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 64\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Convertimos los fragmentos a tensores y generamos embeddings para el primero\n",
    "tensor_chunk = torch.tensor(chunks[0], dtype=torch.long)\n",
    "embeddings = embedding_layer(tensor_chunk)\n",
    "print('Forma de los embeddings:', embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259c3ff",
   "metadata": {},
   "source": [
    "## 6. ¿Por qué los embeddings codifican significado y cómo se relacionan con las redes neuronales?\n",
    "\n",
    "Los embeddings transforman tokens en vectores densos que capturan relaciones semánticas. Esto es posible porque la red neuronal aprende a asignar vectores similares a palabras con significados relacionados durante el entrenamiento. Así, los embeddings permiten que el modelo entienda y procese el significado del texto de manera eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a755d5",
   "metadata": {},
   "source": [
    "## 7. Experimento: Cambiar max_length y stride, analizar resultados\n",
    "\n",
    "Vamos a modificar los valores de `max_length` y `stride` para ver cómo afecta la cantidad de fragmentos generados y el solapamiento entre ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41441a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos diferentes valores de max_length y stride\n",
    "def experimenta_chunks(tokens, max_length, stride):\n",
    "    chunks = chunk_tokens(tokens, max_length, stride)\n",
    "    print(f\"max_length={max_length}, stride={stride} => fragmentos: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "# Caso 1: poco solapamiento\n",
    "chunks1 = experimenta_chunks(tokens, max_length=128, stride=128)\n",
    "# Caso 2: mucho solapamiento\n",
    "chunks2 = experimenta_chunks(tokens, max_length=128, stride=32)\n",
    "# Caso 3: sin solapamiento\n",
    "chunks3 = experimenta_chunks(tokens, max_length=128, stride=128)\n",
    "\n",
    "print('Ejemplo de solapamiento (primeros 2 fragmentos, caso 2):')\n",
    "print(chunks2[0][:20])\n",
    "print(chunks2[1][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ece1e6",
   "metadata": {},
   "source": [
    "### Análisis del experimento\n",
    "\n",
    "Cuando el stride es pequeño, hay más fragmentos y mayor solapamiento, lo que permite que el modelo vea el mismo contexto en diferentes posiciones. Esto es útil para que el modelo aprenda dependencias largas y no pierda información relevante en los bordes de los fragmentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59380c",
   "metadata": {},
   "source": [
    "## 8. Importancia del preprocesamiento y tokenización\n",
    "\n",
    "El preprocesamiento y la tokenización son pasos clave porque convierten el texto en una forma que puede ser entendida por el modelo. Una buena tokenización preserva la información y facilita el aprendizaje de patrones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965a2c92",
   "metadata": {},
   "source": [
    "## 9. Utilidad de la superposición (overlap) en los fragmentos\n",
    "\n",
    "El solapamiento entre fragmentos asegura que la información relevante que cruza los límites de los fragmentos no se pierda, permitiendo que el modelo tenga acceso a más contexto y mejore su comprensión global del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc352e1",
   "metadata": {},
   "source": [
    "## 10. Rol de los embeddings en sistemas agenticos y LLMs\n",
    "\n",
    "En sistemas agenticos y LLMs, los embeddings son fundamentales porque permiten representar información compleja de manera compacta y manipulable, facilitando tareas como búsqueda semántica, razonamiento y generación de texto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
